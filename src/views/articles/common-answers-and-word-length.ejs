<p>Let's take a look at how word length plays a role in which answers get repeated.</p>

<h3>Frequent Crossword Answers</h3>

<p>No matter what what crossword publisher you subscribe to, odds are that after a handful of solved puzzles, you'll start noticing repeated answers. If you've been doing the New York Times puzzles for a while, you and I are probably thinking of the same answers: things like ERA, ORE, and ALOE. In fact, we can take a look. Here are the most frequent answers in the Times crossword since 1993, the year Will Shortz started editing the puzzle:</p>

<table-loader data-id="mostFrequentAnswers" data-src="api/figures/mostFrequentAnswers">
  <form is="query-param-form">
    <label>min answer length <input is="query-param-input" data-query-param="lengthThresh" type="number" value="3"></label>
  </form>
</table-loader>

<p>Repetition isn't necessarily a bad thing, and it's certainly not unique to crosswords. It may seem odd to have predictable answers in a test of general knowledge, but it happens all the time. Pub trivia or quiz bowl regulars probably know which keywords point towards Crécy vs. Agincourt, and fans of Jeopardy! will tell you to study up on Cincinnati slugger Pete Rose to help your chances (his nickname "Charlie Hustle" alone has come up <a href="https://j-archive.com/search.php?search=pete+rose&submit=Search#:~:text=hustle">seven times</a>). Sure, there are only so many semi-obscure facts out there. But I wonder if the repetition is also part of the fun. Call it a guess, call it luck, but sometimes a quick right answer from trained reflex alone feels pretty good.</p>

<p>What sets crosswords apart, though, is that the repeated answers depend a lot on their makeup as <em>words</em>, not just on their nature as trivia. Sometimes you'll come across an answer over and over because of its properties as a word, even though it's pretty linguistically or conceptually obscure. As solvers, we can get frustrated when this happens a lot. We've come up with names for it like "bad fill" and <a href="https://en.wikipedia.org/wiki/Crosswordese">"crosswordese"</a>. You can see it plainly in the table above. The most common words all have a few properties in common <em>as words</em>. I want to look into what those properties are and how we can navigate them as puzzle constructors.</p>

<!-- <p>To sum up: we have a whole mess of crossword answers and an idea that their linguistic properties affect how often they show up in the puzzle. Our set of answers are kind of like a written language - answers correspond roughly to words, and you put those words together according to certain rules of placement to come up with...ok well not meaning or communication per se, but a document of sorts, with an intention behind it. So how do we analyze our language?</p> -->

<h3>Linguistic Laws</h3>

<p>The first electronic collection of language data was published in 1961 as the "Brown University Standard Corpus of Present-Day American English," or the Brown Corpus. Long before then, linguists had been trying to identify, model, and explain patterns in observed language. But with computers at our disposal, it's gotten easier to double check, build upon, generalize, and refine those theories, some of which turn out to apply across a surprising range of language collections (corpora) and contexts. Take Menzerath's law. In 1928, Paul Menzerath suggested that longer German words are often made up of shorter syllables. The concept was refined and generalized to other linguistic phenomena (e.g. longer sentences comprise shorter clauses) by Menzerath himself in 1954 and Gabriel Altmann in 1980. Many such observations have been quantified and codified into similar models -- things like the brevity Law, Zipf's Law, and Heaps' (aka Herdan's) Law.</p>

<h3>Word Frequency</h3>

<p>A number of quantitative linguistics observations relate to word frequency. After all, when you've got a big collection of language, one of the easiest ways to chop it up is by word. And one of the easiest things to do with a whole bunch of words is to count them! It's a lot harder to try and tag things like part of speech, harder still to quantify linguistic contexts. And then there's feature scale -- you could look at syllables or clauses or sentences or...yikes. Still, we shouldn't totally discount the usefulness of word frequency. After all, people have written whole books about it! <a href="https://books.google.com/books/about/Word_Frequency_Studies.html?id=OkDy0RJOwiAC">[REF]</a> <a href="https://www.springer.com/gp/book/9780792370178">[REF]</a>. And it just so happens that word - or answer - frequency is exactly what we're interested in!</p>

<h3>The Brevity Law</h3>

<p>The brevity law advises (qualitatively) that common words tend to be shorter. Check out the table up above and you'll see that the most common words are all three or four letters long. You can play around with the "minimum answer length" input to see how the frequency drops off as length goes up. Looking good so far, but we can do better. A common way to measure something like this is using a correlation test <a href="https://www.mdpi.com/1099-4300/22/2/224/htm">[REF]</a>??. First, we plot every distinct answer's length vs its frequency:</p>

<graph-loader data-id="lengthFrequency" data-src="api/figures/lengthFrequency"></graph-loader>

<p>The x-axis shows answer length. The y-axis shows the log (base 10) of word frequency (so 1 is really 10, 2 is really 100, etc.). The redder a box's color, the more words have that length and that frequency (color is also on a log 10 scale). Right away it seems like we're on to something. As answer length goes up past three, the most common frequency for words of that length goes down. We can run a statistical test to see how monotonic (Spearman) or linearly related (Pearson) the data are -- in other words, how reliably does the graph point roughly down and to the right?</p>

<table-loader data-id="lengthFrequencyCorrelations" data-src="api/figures/lengthFrequencyCorrelations"></table-loader>

<p>Just like we'd expect, both the Pearson and Spearman tests show a significant negative correlation between length and frequency.</p>

<!-- <p>Just like we'd expect, both the Pearson and Spearman tests show a significant negative correlation between length and frequency. The Pearson score ranges from -1 to 1 and says roughly how good a line the data form, with points docked for high standard deviation in either dimension. A score of 0 means the data aren't linearly related at all, whereas our high negative score indicates some negative correlation -- exactly what we're looking for. The Spearman score is a Pearson score on <em>rank</em> data, so it also goes from -1 to 1, and tells us how monotonically our values increase/decrease. Again, it's strongly negative. The large z-values tell us that there's pretty much no chance we're seeing this correlation accidentally, e.g. as a result of sampling error.</p> -->

<!-- <p>We can double check the theory on that by running the same numbers on a corpus of english language data:</p> -->

<!-- <graph-loader data-id="lengthFrequencyEn" data-src="api/figures/lengthFrequencyEn"></graph-loader> -->

<!-- <p>And the correlations:</p> -->

<!-- <table-loader data-id="lengthFrequencyCorrelationsEn" data-src="api/figures/lengthFrequencyCorrelationsEn"></table-loader> -->

<h3>The Probability Mass Function</h3>

<p>I'm going to take a quick detour through the weeds. If you're not interested, feel free to skip ahead to the next heading! As I was looking into word length and frequency, I came across <a href="https://www.mdpi.com/1099-4300/22/2/224/htm">a 2019 paper</a> with a new suggestion about how to quantify the brevity law. It was written by Álvaro Corral and Isabel Serra at the Centre de Recerca Matematica in Barcelona <a href="https://www.mdpi.com/1099-4300/22/2/224/htm">[REF]</a>. They go through a pretty daunting (to me) analysis that culminates in a formulation of the brevity law in terms of conditional distributions of the probability mass function of type length and frequency. To break that down a little, (joint) probability mass is just the likelihood of choosing a word from our corpus with a given length and frequency. So like, "what fraction of answers are five letters long and appear in 12 puzzles?" If that sounds like the histogram above, it should -- the histogram is basically a binned PMF. The "conditional" PMF in this case means the likelihood of choosing a word of a given frequency, where you're choosing from all words of a <em>particular length</em>. So like, "what fraction of 5-letter words appear in 12 puzzles?" For our crossword data, they look like this:</p>

<graph-loader data-id="lengthFrequencyPmfLengthConditionals" data-src="api/figures/lengthFrequencyPmfLengthConditionals">
  <graph-log-axis-checkbox data-checked="true"></graph-log-axis-checkbox>
</graph-loader>

<p>You can hide and show different lines using the legend. One of Corral and Serra's key observations is that these conditional probability distributions are roughly the same shape. If I'm understanding right, they use a scaling analysis to determine how different the shapes are from one another, and say that number is one way to quantify the brevity law. Really I ought to run a scaling analysis on our crossword data too, but just by eyeballing it, things look a little wrong. In particular, the low-frequency domain scales differently to the high. As length increases, the y-intercept goes up, while the x-intercept goes down. This makes a lot of sense to me, and I think maybe the reason it isn't addressed in the paper is that they largely ignore the < 10 frequency domain. Anyway, I'd love to dig into this analysis a little more but I think it's outside of our scope for now, so reach out if you have thoughts!</p>

<p>One last note: the authors also talk about the PMF's <em>marginal</em> distributions, which end up being pretty helpful for understanding what's going on with the brevity law. Marginals are the sum probability for each variable. The frequency marginal basically reduces to Zipf's Law, which you can read more about in its own post! The length marginal is the sum of the PMF across all frequencies for each length. In other words, how likely it is to pick an answer of a given length from our corpus:</p>

<graph-loader data-id="lengthFrequencyPmfLengthMarginal" data-src="api/figures/lengthFrequencyPmfLengthMarginal">
  <graph-log-axis-checkbox data-axes="y"></graph-log-axis-checkbox>
</graph-loader>

<p>It turns out this is a pretty commonly examined distribution in its own right. But before we can talk about it, we need to define a couple of terms.</p>

<h3>Tokens and Types</h3>

<p>Ok so when we say "pick an answer from our corpus" it's not actually so simple. You can count answers two different ways -- either the <em>total</em> number of answers with length, say, four, OR the number of different <em>distinct words</em> with length foour that show up as answers. Do you count OLEO 245 times or just once? You'll hear a range of terms for these concepts, like word "occurrences" (total) vs. "dictionary" words (distinct), or "tokens" (total) vs. "types" (distinct). Here's a graph of tokens and types:</p>

<graph-loader data-id="lengthTypesAndTokens" data-src="api/figures/lengthTypesAndTokens">
  <graph-log-axis-checkbox data-axes="y"></graph-log-axis-checkbox>
</graph-loader>

<p>Remember that the PMF we were looking at earlier was the "probability mass function of <em>type</em> length and frequency?" It used <em>types</em>, distinct words. And we can see that the length marginal distribution from earlier looks exactly like the "types" curve here (if it's hard to make out, try hiding the "tokens" curve by clicking on it in the legend). This tells us about the distribution of entries in the crossword "dictionary." There have been more distinct seven-letter answers than any other length. The tokens curve, on the other hand, counts duplicates multiple times. It peaks at four, meaning the NYT crossword asks you for a four-letter answer more often than for any other length. These two curves show up a fair amount in the literature. According to some papers, they should both fit a lognormal distribution <a href="https://academic.oup.com/biomet/article-abstract/45/1-2/222/264683?redirectedFrom=fulltext">[REF]</a> <a href=" https://royalsocietypublishing.org/doi/10.1098/rsos.191023">[REF]</a>, although others contest this <a href="https://www.mdpi.com/1099-4300/22/2/224/htm">[REF]</a>. Linguists were running this kind of analysis in 1958 <a href="https://www.sciencedirect.com/science/article/pii/S0019995858902298">[REF]</a> and still running it in 2012 <a href="https://arxiv.org/pdf/1207.2334.pdf">[REF]</a>. We can see for ourselves how crosswords compare to a reference English corpus:</p>

<graph-loader data-id="lengthTypesAndTokensCombined" data-src="api/figures/lengthTypesAndTokensCombined">
  <graph-log-axis-checkbox data-axes="y+y2"></graph-log-axis-checkbox>
</graph-loader>

<p>(Aside: choosing a reference corpus is a delicate business. I'm using a <a href="https://github.com/IlyaSemenov/wikipedia-word-frequency">frequency list of english wikipedia words gathered in 2019</a> to represent written english, but there are <a href="https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists">a ton of other great resources</a>. If anyone wants to buy me a COCA license, be my guest!)</p>

<p>The data line up pretty well! Our "types" curves both have a primary peak around seven letters. Our tokens look fairly similar too, with early peaks that fall off pretty quickly. But we can recognize some characteristically "crossword-y" things. Words longer than seven letters are mostly underrepresented in the crossword corpus, which makes sense, since they can be harder to fit into a grid. There are relatively more types of length three-six, which might reflect that crosswords tend to use abbreviations more than a normal corpus. Our tokens distribution is shifted towards towards the long end because crosswords generally don't have one-two letter answers, while one-two letter words are some of the most frequently repeated in English. There are noticeable spikes in the crossword curves at length 15, the width/height of a normal weekday puzzle, and 21, the width/height of a sunday puzzle. There are also smaller spikes at 23 and 25, two other semi-common sizes. As you'd imagine, this is because puzzle creators like to use answers as wide/tall as the grid itself -- check out <a href="https://www.xwordinfo.com/Stacks">this page</a> for some impressive 15-letter-loving puzzles!</p>

<h3>Answer Repetition</h3>

<p>What does all this tell us about repeated answers in the crossword? Well, let's go back to the brevity law. One way to think about it goes like this. There are <em>26<sup>3</sup> = 17,576</em> possible words of length three (using a 26 letter alphabet) and <em>26<sup>6</sup> = 308,915,776</em> possible six-letter words. Let's say in a given corpus there are 1000 three-letter words and 1000 six-letter word. You're choosing your 1000 three-letter words from a smaller pool, so each word has a tendency to show up more times. Shorter words are repeated more often.</p>

<p>We can apply the same thinking to our crossword corpus. Take a look back at the tokens and types curves. Anywhere that the tokens curve is high, we're picking a lot of words of that length. Anywhere that the types curve is low, we don't have all that many distinct words to choose from. The bigger the gap between the two lines, the more times some answer of that length must have been repeated. It doesn't tell us about the distribution -- OLEO could have shown up hundreds of thousands of times and all other four-letter answers just once -- but it's a start!</p>

<p>Toggle on the "tokens - types" trace above to look at this measure of repetition. It's highest at lengths three-five, which should come as no surprise given the table we saw at the beginning of this post. But if you zoom into the long-answer end of the graph, you may notice that the difference doesn't actually hit zero until 21, which means some really long answers have actually showed up more than once! Here's a list of some oft-repeated long answers:</p>

<table-loader data-id="mostFrequentLongAnswers" data-src="api/figures/mostFrequentLongAnswers"></table-loader>

<p>Perhaps as we'd expect, almost all of these are phrases or proper nouns. It would seem that constructors, just like the rest of us, love Leonardo Da Vinci, Arturo Toscanini, and, of course, Grover Cleveland. Just goes to show that language is a little weird in crossword-land. Answers can be made up of many words, etc. And don't forget that a lot of the phenomena we've seen arise partly from the NYT's rules, which specify things like a minimum word count (incentivizing longer words) and diagonal symmetry (perhaps causing a more regular or characteristically shaped word length distribution).</p>

<p>Anyway, that's all for this post, thanks for sticking around til the end!</p>
